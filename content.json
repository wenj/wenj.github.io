{"pages":[{"title":"Jing Wen @ THU CS 五字班","text":"Email: j-wen15@mails.tsinghua.edu.cn Addr: Zijing Student Apartment, Tsinghua University, 100084 前博客链接: Wenj1997’ Blog – Jianshu 目前在cv组里搬砖，时常读读paper，写写代码，做做实验，并努力寻找一些好的idea","link":"/about/index.html"},{"title":"CV","text":"","link":"/cv/index.html"},{"title":"Categories","text":"","link":"/categories/index.html"},{"title":"Links","text":"Zhang Huimeng’s Blog: https://zhanghuimeng.github.io/","link":"/links/index.html"},{"title":"TagCloud","text":"","link":"/tags/index.html"}],"posts":[{"title":"2018-6-29","text":"这周效率不高，周二回学校才开始读paper，结果读到今天又读不进去了。如果顺利的话，在开学前把网络结构这几篇再过一遍。最近读的几篇paper都没有什么惊喜感，不过多读还是有点好处的。我发现读paper的速度提高了一丢丢，这是个好事。实验又出现了一些奇怪的问题，明天去找老师讨论一番。 晚上去听了音乐会，北京交响乐团的，演奏柴一和贝七，最后加演了一首友谊地久天长。感觉柴一一般，贝七还行，大概因为听贝七比较少。 惊闻下周就要开始小学期了，而我的密码学第三次作业还是没写。室友们今天都回来了，不过估计马上他们都要走了。 这周比较伤心的事情是……德国队出局了。其实刚刚知道这个消息心中有点悲愤但也只是有点，结果早上很早很早的时候突然醒了，怎么也接受不了这个事实。现在我倒是接受了，奶一口法国队夺冠。 总之本周比较平淡，由于对刚搭好的博客新鲜感还没有过去，所以随便记了点流水账。周末还有一些工作要做，需要恢复到正常的作息时间了。","link":"/2018/06/29/2018-6-29/"},{"title":"2018-7-12","text":"Summary 小学期进入到第二周了，还剩三周。 上手了v-rep，并完成了给物体生成bounding box，接下来生成数据集应该比较轻松，但是由于暂时用不到先不生成了。 去沈阳办完了签证，直接通过了，没有被check很开心。 查了成绩，没有到理想成绩，不过这是我成绩最好的一个学期了，不过again依旧没办法弥补我上学期屎一样的GPA。（科技史期末论文写了7000+字的ImageNet然后拿了A，很妙。感谢ImageNet！感谢Fei-fei Li！） 看完了切尔诺贝利·禁区。脑洞很大，剧情还说得过去，男女主很好看。去沈阳途中开始看Good Luck Charlie，目前看完了第一季，正在看第二季，我发现这部剧对我的听力和口语有轻微的帮助，而且十分无脑搞笑，适合我。 Todo List 订机票和租房（Important!）并开始准备出发的行李，目前毫无头绪。 De一个大概和v-rep这个软件有点关系的诡异初速度bug。 读YOLO系列，并完成Object Detection的网络。 完成实验室的若干猜想的实验，并重新过一下一些知名网络。 大概最近有一次高中聚餐。 看完Good Luck Charlie，太有趣了。 想去看我不是药神，还没有抽出时间来。","link":"/2018/07/12/2018-7-12/"},{"title":"2018-7-22","text":"Summary 小学期 小学期还剩下最后两周。 这周生成了所有数据，把yolo的官方代码跑起来了，并训练出了一个可以使用的模型。目前正在拿pytorch重新实现yolo。 实验室 开始跑了代码，效果大概还是和假设不符，现在应该快跑完了。 重新读了ResNet和Inception系列，发现之前还是漏了些东西没看。读了Xception，补上了之前不懂的depth-wise convolution。 去CMU相关事宜 被之前的房东鸽了，于是重新找了个住处。 我爸帮我买完了机票。 休息 和sll同学进行了一次愉快的玩耍。我们去了一家日料店，满足心愿吃到了河豚。河豚的口感和想象很不一样，有点脆。店里提供的河豚蘸料是柠檬酱油，吃不习惯，所以最后还是蘸了普通酱油。随后去看了我不是药神，这是最近看的最好的国内电影了。 周五中午和ssh去五道口吃了鱼水饺，味道还是不错的，而且很山东的，很亲切。 看完了Good Luck Charlie。 由于这周下雨比较多，只完成了一次游泳和一次跑步。 用在路上的时间把《禅与摩托车维修艺术》看到了第二部分。摘一段很有趣的观点。 Todo List 小学期 把自己写的yolo调出来。 实验室 先把结果向老师汇报一下，然后思考一下下一步怎么做比较好。 去CMU相关事宜 订家具，主要是床。 收拾行李。 休息 打算继续阅读《禅与摩托车维修艺术》。 这次是真正意义上的周总结了，因为写完这个就要洗漱睡觉了。这周在机房的时间效率还可以，但是在寝室基本什么正事都没干。所以希望下周的效率能再高一点。 我决定开始认真地减肥和生活，首先从加快每件事的实现速率、减少花在社交网络的时间、提高学习时间、规律运动以及提高洗衣服频率做起。","link":"/2018/07/22/2018-7-22/"},{"title":"Covariance Pooling for Facial Expression Recognition","text":"Link：arXiv:1805.04855 这又是一篇尝试将二阶统计量用于神经网络的paper，应用场景是表情识别。在文中，作者给出了如何使用covariance pooling做图像的表情识别和视频的表情识别。 表情识别更依赖于面部关键点的扭曲程度而不是是否存在某个关键点，因此作者认为二阶统计量相比于一阶统计量更容易捕捉到面部特征的扭曲程度，更适用于表情的识别。除此之外，covariance pooling还可以捕捉特征在各个帧之间的变化，因此可以被用在视频的表情识别中。 相较于之前使用协方差矩阵的工作，这篇文章在协方差矩阵的基础上又使用了降维层和non-linear rectification层。这一部分使用了后面提到的manifold network，它主要有两个作用：(1)由于协方差矩阵flatten后直接接全连接层过大，这部分可以起到降维的作用；(2)可以保留原矩阵中的几何信息。我猜测covariance pooling中的pooling是指降维这一部分。但是有一点有待进一步思考：Pooling最主要的作用是不变形还是降维，covariance pooling中是否有能体现不变形的部分？ 模型对于图像的表情识别，分成以下几步： 由于图像中存在很多不相关信息，首先进行脸部识别，然后根据关键点将人脸摆好。 接下来将人脸通过CNN输出特征。 使用CNN得到的特征做covariance pooling，并且把covariance matrix输入到后面接的manifold network中学习深度二阶统计量。 视频中的表情识别和图像的表情识别类似： 在视频中抽取出一些帧，进行脸部识别。 将各帧中的人脸串接起来输入到3D卷积中。 将3D CNN得到的特征做covariance pooling，并把covariance matrix输入到manifold network中。注意这一步和图像不同，这里的协方差矩阵求的是各帧的特征之间的协方差，也即时间轴上的协方差矩阵。 因此可以看到，核心就是covariance pooling和manifold network部分。 Covariance Pooling文中对于covariance pooling的描述如下： 给定一组特征 \\(f_1, f_2, …, f_n\\in \\mathbb{R}^d\\)是一组特征，他们的covariance matrix为$$\\mathbf{C}=\\frac{1}{n-1}\\sum_{i=1}^n(\\mathbf{f_i}-\\mathbf{\\bar f})(\\mathbf{f_i}-\\mathbf{\\bar f})^T$$其中，\\(\\mathbf{\\bar f}=\\frac{1}{n}\\sum_{i=1}^n\\mathbf{f_i}\\)。 对于图像中的表情识别，我们这样定义\\(f_1, f_2, …, f_n\\)：令\\(\\mathbf(X)\\in \\mathbb{R}^{w\\times h \\times d}\\)，其中\\(w,h,d\\)分别是卷积层输出的width，height和channel数。将\\(\\mathbf{X}\\) flatten成一个\\(n\\times d\\)的矩阵命名为\\(\\mathbf{X’}\\)，那么\\(f_1, f_2, …, f_n\\)就是\\(\\mathbf{X’}\\)的各列。 其实简单来看，就是将卷积层的输出中每个channel flatten成一个向量，对d个向量求协方差矩阵。 对于视频中的表情识别，用来做协方差矩阵的不再是各个channel的向量，而是从不同帧中提取出来的特征向量。细节还没有仔细研究。 SPD Manifold Network (SPDNet) Layers这部分内容来自《A Riemannian Network for SPD Matrix Learning》（arXiv:1608.04233）。SPDNet将对称正定矩阵作为输入学习出新的特征。 考虑上一部分定义的协方差矩阵\\(\\mathbf{C}\\)可能是一个对称半正定矩阵，为了符合SPDNet的输入要求，将其变成以下对称正定矩阵$$\\mathbf{C^+}=\\mathbf{C}+\\lambda trace(\\mathbf{C})\\mathbf{I}$$其中，\\(\\lambda\\)是正则化参数，\\(\\mathbf{I}\\)是单位矩阵。 SPDNet中主要有三层： Bilinear Mapping Layer (BiMap)：考虑到协方差矩阵flatten后直接接全连接层的话，全连接层输入过多，因此可以用这一层来进行降维。另一方面，这一层可以保留原矩阵的几何信息。设\\(\\mathbf{X_{k-1}}\\)是该层的输入，\\(\\mathbf{W_k}\\in \\mathbb{R}_*^{d_k \\times d_{k-1}}\\)是权重矩阵，那么输出\\(\\mathbf{X_{k}}\\in \\mathbb{R}^{d_k \\times d_k}\\)定义为$$\\mathbf{X_k}=f_b^k(\\mathbf{X_{k-1};\\mathbf{W_k}})=\\mathbf{W_k}\\mathbf{X_{k-1}}\\mathbf{W_k}^T$$ Eigenvalue Rectification (ReEig)：这一层有点类似于ReLU，可以引入非线性。设输入为\\(\\mathbf{X_{k-1}}\\)，对输入做特征值分解得到\\(\\mathbf{X_{k-1}}=\\mathbf{U_{k-1}}\\Sigma_{k-1}\\mathbf{U_{k-1}}^T\\)，那么输出\\(\\mathbf{X_{k}}\\)定义为$$\\mathbf{X_k}=f_r^k(\\mathbf{X_{k-1}})=\\mathbf{U_{k-1}}max(\\epsilon \\mathbf{I}, \\sigma_{k-1})\\mathbf{U_{k-1}}^T$$其中，max操作是逐元素取max。 Log Eigenvalue Layer (LogEig)：按照文中的说法，这一部分的作用是“给黎曼流形中的元素赋予李群结构，使得矩阵被flatten后可以使用标准的欧式运算”。这个原理超出了我的知识范围，因此目前还没有理解。好在理论难但是做法简单。设输入为\\(\\mathbf{X_{k-1}}\\)，对输入做特征值分解得到\\(\\mathbf{X_{k-1}}=\\mathbf{U_{k-1}}\\Sigma_{k-1}\\mathbf{U_{k-1}}^T\\)，那么输出\\(\\mathbf{X_{k}}\\)定义为$$\\mathbf{X_k}=f_l^k(\\mathbf{X_{k-1}})=\\mathbf{U_{k-1}}log(\\Sigma_{k-1})\\mathbf{U_{k-1}}^T$$ 把BiMap和ReEig两层用BiRe表示，那么SPDNet结构如下： 实验文章中主要用了Static Facial Expressions in the Wild (SFEW) 2.0 dataset和Real-world Affective Faces (RAF) dataset连个数据集，前者用于图像表情识别，后者用于视频表情识别。 实验中比较了使用不同的baseline训练或finetune的结果和使用covariance pooling得到的结果，使用covariance pooling可以提升三四个点。实验中还比较了covariance pooling+SPDNet中一些参数的影响，比如SPDNet后使用多少个全连接层以及SPDNet中BiRe层的个数等等，实验结果详见paper。 总结看了几篇关于如何在网络中使用二阶统计量后发现，貌似大多数paper到求covariance matrix那里都是一致的，重点在于得到covariance matrix后要怎么处理。针对这篇文章来说，就是使用SPDNet来处理covariance matrix。 对于这篇paper，还有一些细节没有搞清楚： Covariance pooling的pooling是怎么体现的。按照模型部分的描述，covariance pooling貌似只求了一个covariance matrix。 SPDNet的理论，这个超出了我的数学知识，不太能读懂。 关于视频中的表情识别文中描述的比较简略，细节不清楚。","link":"/2018/06/27/acharya18-covariance-pooling/"},{"title":"Vrep入门：使用Vrep实现一个可以转动的豌豆射手","text":"今天初步入门了一下Vrep，为了防止忘记踩过的坑和一些细节，记录一下。 为了快速使用Vrep，我拿出了几个月没有用过的鼠标。在开始之前，先介绍三个基本的鼠标操作： 按住鼠标左键拖动，可以移动视图。 滚动鼠标滚轴，可以放大缩小视图。 按住中键拖动，可以改变视角。（这大概是我第一次用到这个鼠标操作……） 打开Vrep，新建一个视图，可以看到这样一个界面： 场景里有一个水平面，右下角还有一个小小的直角坐标系。按住中键拖动时，这个直角坐标系也会随之旋转。实现旋转功能非常简单，只需要转动轴和物体即可。首先创建一个球体（或正方体等whatever）。点击Add-&gt;Primitive Shape-&gt;Sphere，在弹出的对话框中可以设置球的半径等参数。确认之后场景中出现一个球体，貌似在默认情况下球下边沿就是和水平面对齐了的。 接下来创建一个旋转关节，点击Add-&gt;Joint-&gt;Revolute。 通过菜单栏中的object/item shift和object/item rotate将旋转关节和球体的中轴重合（即x和y坐标设为一致的，z坐标只需将关节下边沿和水平面对齐即可）。 一个方便的对齐方法如下。同时选中关节和球体，将一个的位置应用到另一个。 接下来设置两个物体的层级关系，在Scene hierarchy中将Sphere拖入Revolute_joint。 之后，为了能够使用lua脚本给旋转关节设置速度，建立一个随便什么物体，使旋转关节和球体层级关系上属于这个物体。右键Scene hierarchy中这个物体，Add-&gt;Associated Child Script-&gt;Non threaded。之后这个物体名字右侧会出现一个小的文件图标，双击这个图标就可以查看并修改lua脚本。 在lua脚本中，sysCall_init函数添加以下两句。12v0=1handle=sim.getObjectHandle('Revolute_joint') 在sysCall_actuation函数中添加以下一句给关节设置速度。1sim.setJointTargetVelocity(handle,v0) 双击Scene hierarchy中旋转关节的图标，在属性中点击最下的Show dynamic properties dialog，然后在新弹出的对话框中，选中Motor enabled。理论上现在就可以转动了。 最后一步，把豌豆射手的网格模型贴上去。网格模型是我从网上下载的STL模型，通过File-&gt;Import-&gt;Mesh导入即可。为了美观，让这个网格模型叶子的中心和关节的x、y轴对齐。效果差不多是这个样子。 然后将网格模型从属于球体。运行之后，这个豌豆射手就可以旋转了。最后我们将除网格之外的所有东西隐藏掉（可以看到Scene Hierarchy中名字变暗了），就可以实现一个旋转的豌豆射手了。隐藏物体时，把对勾全部消掉。 最后效果如下：(function(){var player = new DPlayer({\"container\":document.getElementById(\"dplayer0\"),\"video\":{\"url\":\"/images/vrep.mp4\"},\"danmaku\":{\"api\":\"https://api.prprpr.me/dplayer/\"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()","link":"/2018/07/03/vrep-rotate-peashooter/"},{"title":"机器人小学期（1）：YOLOv1","text":"机器人小学期（1）：YOLOv1 机器人小学期（2）：YOLOv2 机器人小学期（3）：YOLOv3 机器人小学期（4）：将YOLO应用到自己的数据集上 机器人小学期（5）：使用PyTorch实现YOLOv3-tiny 原文链接：You Only Look Once: Unified, Real-Time Object Detection 机器人小学期做了一个奇怪的项目，大概是用v-rep这个平台仿真了一个简化版植物大战僵尸（当然，除了有植物有僵尸并且植物会打僵尸，僵尸试图吃点植物外没有什么相同之处）。事实证明使用一个机器人仿真平台来搞游戏虚拟器还是非常灾难的。 我在项目里负责目标检测部分，简单来说，就是给植物装了一个视觉传感器，我要使用视觉传感器传回来的图像，检测出图像中每个僵尸的bounding box。由于对检测速度要求比较高，因此就使用了YOLO这个网络。 YOLO已经有了三版（v1、v2、v3）。每一个版本在上一版本的基础上做出了一些修改，使得速度更快，效果更好。本文只包括YOLOv1，也就是最早的一版，和最新的YOLOv3相比，还是有不少差异的。 YOLOv1回忆R-CNN的做法，R-CNN首先使用regional proposal方法选出一些可能的bounding box，然后对这些bounding box做分类，最后再对这些bounding box做筛选、微调。R-CNN把object detection这个任务分成了几个部分，包括选择bounding box和分类等，每部分都由一个专门的网络负责。这样带来的问题是，速度慢并且很难优化。因此YOLO针对这个问题作出了改进：在YOLO里，我们将整张图片作为输入，获得bounding box和分类都在同一个网络中完成（look once），也就是说我们把一整个detection部分看成了一个回归问题。这个做法带来的好处包括： 速度快：Titan X上可以做到45fps，fast version可以做到150fps。 直接使用整张图片使得我们可以获得更多的全局信息，相比于Fast R-CNN，YOLO的背景识别错误更少。 泛化能力更强。 网络结构YOLOv1有两种网络，一个是24个卷积层的YOLO，还有一个简化版的只有9个卷积层的Fast YOLO。下面以24个卷积层的版本为例。 网络包含24个卷积层和2个全连接层，通过卷积层提取特征，然后用全连接层预测概率和坐标。 在具体实现中，除了最后一层的激活函数为ReLU外，其他各层的激活函数均为leaky ReLU，定义如下： $$\\phi(x)=\\begin{cases}x, &amp;\\text{if $x&gt;0$} \\\\0.1x, &amp;\\text{otherwise} \\\\end{cases}$$ 输入和输出网络将整个图片作为输入，最后输出一个大小为\\(S*S*(B*5+C)\\)（注意Tensor大小与v3有所不同）的Tensor。 我们对于这个输出Tensor做一点解释。我们将输入图像划分成\\(S*S\\)的网格，对于网格中的每个格子，我们要预测\\(B\\)个中心落在这个格子里的bounding box。每个bounding box有5个参数，分别是中点的\\(x\\)、\\(y\\)坐标以及宽度\\(w\\)和高度\\(h\\)，此外，还包含一个置信值\\(Pr(Object)*IOU_{pred}^{truth}\\)表示这个bounding box里包含一个物体的概率乘上它和真实值的IOU。最后，对于每个格子，我们还要预测\\(C\\)个参数，其中第\\(i\\)个参数\\(Pr(C_i|Object)\\)表示在格子里有物体的情况下，这个物体属于第\\(i\\)类的概率。 根据置信值和属于各类别的概率，我们可以求出各类别的置信值： $$Pr(Class_i|Object)*Pr(Object)*IOU_{pred}^{truth}$$ 额外说明一点，\\(x\\)、\\(y\\)是相对这个格子的坐标，并且归一化到了\\([0,1]\\)区间；宽度\\(w\\)和高度\\(h\\)是相对于正常图片的，也归一化到了\\([0,1]\\)区间。 损失函数最后，我们还需要定义损失函数。 为了便于优化，我们使用误差的平方和作为损失函数，但是这会带来两方面的问题： 定位错误和分类错误同等对待。 在一张图片中往往有很多格子是不包含物体的，这写格子的置信值会逐渐趋于0，这会淹没那些包含物体的格子的误差。（置信值confidence score为为\\(Pr(Object)*IOU_{pred}^{truth}\\)。） 不同尺寸的物体在计算错误时也被平等对待了。但是很明显，相同的bounding box的误差出现在预测尺寸较大的物体可能并不显眼，但是如果出现在预测尺寸较小的物体上时就会很严重。 为了解决前两个问题，我们赋予定位错误和预测是否包含物体的错误不同的权重，其中定位错误的权重\\(\\lambda_{coord}=5\\)，预测是否包含物体的错误的权重\\(\\lambda_{noobj}\\)。 为了解决第三个问题，我们不直接使用bounding box的高度\\(w\\)和宽度\\(h\\)计算，而是使用它们开根号后计算误差。 最后，我们得到一个形如下式的损失函数， $$\\begin{align} &amp;\\lambda_{coord}\\sum_{i=0}^{S^2}\\sum_{j=1}^{B}\\mathbb{1}_{ij}^{obj}[(x_i-\\hat{x}_i)^2+(y_i-\\hat{y}_i)^2] \\\\ +&amp;\\lambda_{coord}\\sum_{i=0}^{S^2}\\sum_{j=1}^{B}\\mathbb{1}_{ij}^{obj}[(\\sqrt{w_i}-\\sqrt{\\hat{w}_i})^2+(\\sqrt{h_i}-\\sqrt{\\hat{h}_i})^2] \\\\ +&amp;\\sum_{i=0}^{S^2}\\sum_{j=1}^{B}\\mathbb{1}_{ij}^{obj}(C_i-\\hat{C}_i)^2+\\lambda_{noobj}\\sum_{i=0}^{S^2}\\sum_{j=1}^{B}\\mathbb{1}_{ij}^{obj}(C_i-\\hat{C}_i)^2 \\\\ +&amp;\\sum_{i=1}^{S^2}\\mathbb{1}_{i}^{obj}\\sum_{c\\in classes}(p_i(c)-\\hat{p}_i(c))^2 \\end{align}$$ \\(\\mathbb{1}_{ij}^{obj}\\)表示第\\(i\\)个格子里的第\\(j\\)个预测“负责”预测这个物体，\\(\\mathbb{1}_i^{obj}\\)表示物体\\(obj\\)出现在第\\(i\\)个格子里。（“负责”预测这个物体是指这个预测的bounding box和某个真实值的IOU是所有预测中最大的，IOU为Intersection of Units，即两个bounding box面积的交/并。）","link":"/2018/07/23/yolo-series-1/"},{"title":"机器人小学期（2）：YOLOv2","text":"机器人小学期（1）：YOLOv1 机器人小学期（2）：YOLOv2 机器人小学期（3）：YOLOv3 机器人小学期（4）：将YOLO应用到自己的数据集上 机器人小学期（5）：使用PyTorch实现YOLOv3-tiny 原文链接：YOLO9000: Better, Faster,Stronger YOLO9000是YOLO系列的第二版，在后面都是用YOLOv2表示。在YOLO的基础上，YOLOv2提出了诸多改进，核心就是让YOLO更快、更准确。YOLOv2中的模型YOLO9000号称可以检测9000个类别，并且可以支持一些类别没有detection label的情况下，来进行detection的工作，当然这部分的准确率还不算太高。 在这篇博客里，我将按照论文里的逻辑，分成Better、Faster和Stronger三个部分来描述论文里提出的改进。 BetterYOLOv2和YOLOv1相比，准确率得到了一些提升。在YOLOv1中比较显著的localization error较高以及recall低的问题，在YOLOv2中都得到了一些改善。为了使模型准确率更高，YOLOv2做出了以下改变： Batch Normalization：YOLOv2在所有卷积层后面加了batch normalization，并删除了原来的dropout层，这使得mAP有2%的提升。 High Resolution Classifier：在YOLOv2中，使用了448*448的分辨率。训练时，首先在ImageNet上对分类网络finetune 10个epoch，然后再finetune detection部分。这个操作使得mAP上涨4个点。 Anchor Boxes and Prediction：YOLOv1中使用全连接层来直接预测bounding box，但在YOLOv2中，我们改用anchor boxes来预测bounding box。 如果我们打开yolo的代码仓库，找到里面的.cfg文件，可以看到在网络配置里有这么一行（以yolov2-tiny.cfg为例）。 1anchors = 0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828 这就是yolov2-tiny网络中使用的anchor，这其中每两个为一对，分别对应于x方向和y方向，上面这行显示的就是5个anchor。 那么这些anchor是怎么生成的呢？首先我们拿到训练数据里所有bounding box的宽度和高度\\((w,h)\\)，并假设这些bounding box的中心都在同一个点上，这样我们就得到了一组以同一个点为中心的长方形，对这些\\((w,h)\\)做k-means clustering，就可以得到\\(k\\)个聚类中心，也就是我们要的\\(k\\)个anchor，在实验中，我们发现\\(k=5\\)时的效果最好。为了更符合我们的任务，做k-means clustering时不使用欧几里得距离作为距离的衡量，而是使用IOU的大小作为距离的衡量，也即：$$d(\\text{box},\\text{centroid})=1-\\text{IOU}(\\text{box},\\text{centroid})$$ 现在，我们获得了所有的anchor，接下来，我们需要调整网络的输出。和YOLOv1一样，如果输入图像分辨率为416*416，我们可以将输入图像划分为13*13个格子，其中每个格子大小为32*32，这样我们就会输出一个13*13的feature map，其中feature map的每一维也是一个向量，对应于中心点落在这个格子里的那些bounding box。在YOLOv2中，我们在每个格子里预测出5个bounding box，也就是五组中心坐标\\((b_x,b_y)\\)，宽度\\(b_w\\)，高度\\(b_h\\)以及置信值\\(Pr(\\text{Object})*\\text{IOU}(b, \\text{Object}))\\)。值得注意的是，YOLOv2中不在直接输出这5个数，而是输出变化前的5个值\\(t_x,t_y,t_w,t_h,t_o\\)，然后经过变换 $$\\begin{align}b_x&amp;=\\sigma(t_x)+c_x \\\\b_y&amp;=\\sigma(t_y)+c_y \\\\b_w&amp;=p_we^{t_w} \\\\b_h&amp;=p_he^{t_h} \\\\Pr(\\text{Object})*\\text{IOU}(b, \\text{Object}))&amp;=\\sigma(t_o)\\end{align}$$ 就可以得到\\(b_x,b_y,b_w,b_h,Pr(\\text{Object})*\\text{IOU}(b, \\text{Object}))\\)。其中\\(\\sigma(x)\\)表示Sigmoid函数，\\((c_x,c_y)\\)是这个格子左上角的坐标，\\(p_w,p_h\\)就是我们直接求出的anchor，5个bounding box正好对应于5个不同的anchor。 Fine-Grained Features：修改后的YOLOv2在13*13的特征图上进行预测，但是这样的分辨率可能和对于小尺度不利。Faster R-CNN和SSD在不同的特征图上跑proposal network来获得不同的分辨率，但在YOLOv2里，我们直接使用一个passthrough layer将前面层里的26*26分辨率的特征图和低分辨率的特征图用类似于ResNet中连Identity Mapping的方法连起来（我们把26*26*512特征图转换成13*13*2048，然后就可以和13*13的特征图连起来）。然后我们把这个特征图上做检测。这可以使performance提高1%。 Multi-Scale Training：YOLOv2把416*416作为输入的分辨率，但是由于网络里只有卷积核和max pooling，因此希望YOLOv2在应对不同分辨率的输入时有鲁棒性。我们可以在训练里不固定输入的大小，而是每10个epoch就随机选取另一个大小的图像，由于我们在网络里降采样的倍数是32，所以我们让所有的分辨率都是32的倍数（{320; 352; …; 608}），这样网络就需要不停的学习预测不同分辨率的图像。 在低分辨率的图像中，YOLOv2可以达到90FPS的速度，mAP和Fast R-CNN差不多。在高分辨率的图像中，YOLOv2仍可以达到实时的速度，78.6mAP。 FasterYOLOv2没有使用VGG-16作为baseline，而是构建了一个新网络Darknet-19，这个网络包含19和卷积层和5个maxpooling，结构如下： StrongerYOLOv2使用一种机制，可以同时训练classification和detection两部分的数据。细节还没有研究清楚。","link":"/2018/07/25/yolo-series-2/"},{"title":"机器人小学期（3）：YOLOv3","text":"机器人小学期（1）：YOLOv1 机器人小学期（2）：YOLOv2 机器人小学期（3）：YOLOv3 机器人小学期（4）：将YOLO应用到自己的数据集上 机器人小学期（5）：使用PyTorch实现YOLOv3-tiny 原文链接：YOLOv3: An Incremental Improvement YOLOv3是YOLO系列的最新版本，总体来讲变化不算特别大。（I didn’t do a whole lot of research this year. Spent a lot of time on Twitter. Played around with GANs a little. ——作者如是说）但是为了完成这个项目，我还是读了一遍paper。事实上，参考文献中列出了一个非常详细YOLOv3的教程，教程中包括了一些关于YOLOv3的细节说明，并手把手教读者如何从cfg文件开始构建网络，加载模型参数并完成预测。这个教程使用PyTorch实现的，非常有参考意义。美中不足的是，这个教程只包含了detection部分，而没有告诉你如何使用PyTorch从头开始训练网络。 在这篇博客中，我依旧按照paper中的逻辑讲解YOLOv3，更多的细节可以在后面的文章中看到。 YOLOv3预测Bounding Box在YOLOv3中，我们依旧使用类似于YOLOv2的方法预测bounding box，对于每个bounding box，给出四个参数\\(t_x,t_y,t_w,t_h\\)，然后通过变换 $$\\begin{align}b_x&amp;=\\sigma(t_x)+c_x \\\\b_y&amp;=\\sigma(t_y)+c_y \\\\b_w&amp;=p_we^{t_w} \\\\b_h&amp;=p_he^{t_h} \\\\\\end{align}$$ 就可以得到bounding box的真实值。关于参数的解释，详见上一篇文章，这里不再解释。 在训练中，我们使用误差的平方和作为loss。如果真实值是\\(\\hat{t_{*}}\\)而预测值是\\(t_{*}\\)，那么误差就是\\((\\hat{t_{*}} -t_{*}\\)。需要额外说明的是和bounding box有关的第五个参数\\(t_o\\)，在YOLOv3中这个参数依旧表示置信值\\(Pr(\\text{Object})*\\text{IOU}(b, \\text{Object}))\\)。\\(t_o\\)的真实值\\(\\hat{t}_o\\)是这样定义的：如果这个\\(t_o\\)对应的bounding box prior和某个物体的真实的bounding box重叠的比例高于其他bounding box prior，那么\\(\\hat{t}_o=1\\)，我们称之为这个bounding box prior分配给某个物体的真实值。（注：按照我的理解，bounding box prior和真实的bounding box的重叠比例是这么得到的。和YOLOv2相同，YOLOv3中的每个prediction都对应于一个anchor，这个anchor将用于计算bounding box的宽度和高度。bounding box prior以anchor作为宽度和高度，以真实bounding box的中心作为中心，然后就可以求bounding box prior和真实bounding box的重叠比例了。） 在计算loss的时候，如果一个预测和某个真实物体的bounding box重叠超过某个threshold（0.5），那么我们忽略掉这个预测的loss。如果一个预测和某个真实物体的bounding box prior没有被分配给某个物体的真实值，那么我们只将置信值\\(t_o\\)的误差加入loss，而忽略掉分类误差和定位误差。 预测分类考虑到使用softmax作为分类器时，我们假设了每个bounding box只包含一个类别的物体，而实际上每个bounding box可能包含多个类别的物体，因此我们对每个类别使用独立的逻辑分类器。我们使用binary cross-entropy loss作为预测分类的loss。 基于不同尺度的预测YOLOv3依旧使用一组anchor来辅助不同尺度的预测，不同之处在于，YOLOv3使用了9个anchor，将这9个anchor分成3组看作3个不同的尺度。在每个预测层中，我们只使用其中一组3个anchor，相应的每个格子给出3个预测bounding box。 我们按照如下方法使用这3个不同的尺度。首先在基础特征提取器上，我们连接几个卷积层，然后接第一个尺度的预测层。然后，取出之前两层的特征图升采样两倍，再和之前某一层的特征图接起来，再通过几个卷积层，接入下一个尺度的预测层。以此类推。 特征提取YOLOv3融合了YOLOv2中Darknet-19和ResNet的想法，提出了新的特征提取器，如图。 效果 无效的尝试作者在文中还提到了一些尝试过但效果不好的做法： 使用anchor box预测机制来预测anchor box的x和y偏移。 使用linear activation作为x和y预测的激励而不是使用logistic activation。 使用focal loss。 使用双IOU threshold。当一个预测和某个真实值的threshold大于0.7或小于0.3时，考虑这个预测中\\(t_o\\)的误差。这个操作会使得mAP降低。","link":"/2018/07/26/yolo-series-3/"},{"title":"机器人小学期（4）：将YOLO应用到自己的数据集上","text":"机器人小学期（1）：YOLOv1 机器人小学期（2）：YOLOv2 机器人小学期（3）：YOLOv3 机器人小学期（4）：将YOLO应用到自己的数据集上 机器人小学期（5）：使用PyTorch实现YOLOv3-tiny YOLO主页上提供了很详细的教程，讲解如何使用作者提供的YOLO实现，其中包括如何使用pretrain的model，如何在VOC和COCO数据集上训练。在这篇文章中，我们来看一下如何能将作者提供的YOLO实现应用到自己的数据集上。 安装YOLO首先，我们需要跟着主页上的教程安装YOLO。123git clone https://github.com/pjreddie/darknetcd darknetmake 如果想使用GPU的话，要在make之前把Makefile中前两行置1。12GPU=1CUDNN=1 调整我们的数据集在此链接中有我制作的数据集。我们来看一下数据集里都需要什么内容： 训练集和验证集的图片images/ 训练集和验证集的label labels/：目录下的每个label文件名和图片对应，把图片文件名的格式.png/jpg/...改成.txt。label文件中，从第一行开始每行表示一个bounding box。每个bounding box包含5个参数，依次是类别（从0开始），x，y，w，h。其中(x,y)是bounding box的中心点位置，w表示bounding box的宽度，h表示bounding box的高度。注意，后四个参数都要除以图片的尺寸以获得[0,1]的数。 10 0.4140625 0.51953125 0.125 0.17578125 一个文本文件，包含了训练集中所有图片的路径train.txt： 12345/home/robot/Desktop/PVZ/data/images/1.png/home/robot/Desktop/PVZ/data/images/2.png/home/robot/Desktop/PVZ/data/images/3.png/home/robot/Desktop/PVZ/data/images/4.png...... 一个文本文件，包含了验证集中所有图片的路径valid.txt：和train.txt类似。 一个文本文件，包含了所有类别的名字zombie.name：从第一行开始每行写一个名字，第一行为第0类的名字，第二行为第1类的名字，以此类推。比如在我们的数据集中只有一类，这类名字叫Marty，因此只有一行，上面写着Marty。1Marty 到现在为止，我们的数据集就已经创建好了，接下来我们要做的就是把数据喂进YOLO中进行训练。 使用YOLO训练接下来，我们来看一下如何训练。由于我在项目中使用的是YOLOv3-tiny这个网络，所以下面都以这个网络为例。 在darknet/cfg/中有很多.cfg文件，这个文件描述了网络结构和一些训练参数，因为darknet实现了一套自己的底层框架，所以如果你先构建新的网络，也需要按照这个格式来。 打开darknet/cfg/yolov3-tiny.cfg文件，为了能使用我们自己的数据集，还要对其做一些调整。为了方便起见，我们创建两个.cfg文件，yolov3-tiny.cfg用于训练和yolov3-tina-testing.cfg用于测试。 在yolov3-tiny.cfg文件中我们将[net]中的batch size设为64，subdivision设为2。在yolov3-tiny.cfg-testing文件中我们将[net]中的batch size设为1，subdivision设为1。yolov3-tiny.cfg： 123# Testingbatch=1subdivisions=1 yolov3-tiny-testing.cfg： 123# Testingbatch=64subdivisions=2 根据自己的数据集调整[net]中的width和height，比如在我的数据集中图片是512*512的，所以有 12width=512height=512 接下来，我们要把所有yolo层里的classes改成自己的数据集中的类别数。比如我的数据集中只有一类，因此classes=1。 最后也是最重要的一步，我们需要对每个yolo层前的卷积层的输出channel数做调整。根据上一篇文章，我们知道输出时每个格子对应的channel为\\3*(4+1+num_classes\\)，所以要把filters改为这个数。比如在我的数据集中，只有一类，那输出channel数为18，因此改成filters=18。 在修改完网络后，我们还需要建立一个新文件zombie.data，里面包含了训练时所用的一些目录的信息：12345classes = 1train = /home/robot/Desktop/PVZ/data/train.txtvalid = /home/robot/Desktop/PVZ/data/valid.txtnames = /home/robot/Desktop/PVZ/data/zombie.namebackup = backup 其中classes表示数据集中的类别数目，train指向上一节提到的train.txt所在位置，valid指向上一节提到的valid.txt所在位置，names指向类别名字的文件zombie.name所在位置，backup表示中间存储的目录，之后训练过程中存储下来的weights文件都可以在这个目录下找到。 现在我们就可以开始训练了，参照主页中的格式运行darknet：1./darknet detector train zombie.data yolov3-tiny.cfg darknet.conv.weights 最后一个命令行参数是pretrain的权重文件，关于YOLOv3-tiny的pretrain model可以在此链接处下载。 训练好后，我们在backup的目录下找到weights文件，然后进行测试：1./darknet detector test zombie.data yolov3-tiny-testing.cfg backup/[weights file] 然后就可以看到目录下出现了一个predictions.jpg文件，如图：","link":"/2018/08/01/yolo-series-4/"},{"title":"机器人小学期（5）：使用PyTorch实现YOLOv3-tiny","text":"机器人小学期（1）：YOLOv1 机器人小学期（2）：YOLOv2 机器人小学期（3）：YOLOv3 机器人小学期（4）：将YOLO应用到自己的数据集上 机器人小学期（5）：使用PyTorch实现YOLOv3-tiny 首先，我们先来回忆一下YOLO网络在做什么。 现在，我们有一张大小为512*512的狗的图片，现在我想预测出狗的bounding box。我们先把图片按照步长32划分成16*16的网格，通过YOLO网络，这个网格中的每个格子都会预测出3个bounding box，这3个bounding box的中心都落在这个格子里。如果网络效果不错的话，红色格子对应的3个预测里就有一个是狗的bounding box（也就是黄色框）。对于每一个bounding box，我们都会预测出(4+1+n)个参数，其中前面4个参数和bounding box的中心坐标以及长宽有关，第五个参数表示这个bounding box里有物体的概率和这个bounding box和真实bounding box的IOU的成绩，最后n个参数表示如果这个bounding box里有物体，那么这个物体属于某一类的概率有多大，也就是分类任务的输出。 因此，对于这张图片，我们会预测出16*16*3个bounding box，首先我们先根据第五个参数，筛出可能包含物体的bounding box。然后使用非最大抑制（non-maximum suppression）找到这些bounding box里比较准确的那些bounding box。 使用的YOLOv3-tiny网络结果如图所示，其中蓝色的部分用于特征提取，然后经过几个卷积层的调整接到YOLO层来预测出小尺度的bounding box。此外，我们特征提取部分的特征上采样然后和更浅的层的特征连接起来用于预测尺度大一些的bounding box。由于训练过程中观察到网络不太好收敛，所以可以先不管右侧这个分支，只训练左侧这一路，经过3000个epoch之后，再训练整个网络，这样可以使网络收敛得更快一些。","link":"/2018/08/01/yolo-series-5/"}],"tags":[{"name":"Second-Order Statistics","slug":"Second-Order-Statistics","link":"/tags/Second-Order-Statistics/"},{"name":"Facial Expression Recognition","slug":"Facial-Expression-Recognition","link":"/tags/Facial-Expression-Recognition/"},{"name":"v-rep","slug":"v-rep","link":"/tags/v-rep/"},{"name":"Object Detection","slug":"Object-Detection","link":"/tags/Object-Detection/"}],"categories":[{"name":"Daily & Weekly","slug":"Daily-Weekly","link":"/categories/Daily-Weekly/"},{"name":"实验室科研探究","slug":"实验室科研探究","link":"/categories/实验室科研探究/"},{"name":"课堂","slug":"课堂","link":"/categories/课堂/"}]}